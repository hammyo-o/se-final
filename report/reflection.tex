\documentclass[12pt]{article}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{setspace}
\usepackage{times}

\title{Reflection Report: MCP Automated Testing Agent}
\author{Hammy Siddiqui and Leonardo Diaz}
\date{}

\begin{document}
\maketitle

\begin{abstract}
This reflection explains how we built an MCP agent that runs Maven tests, manages Git commands, and generates specification-based tests automatically. We describe how we approached the problem, what kind of improvements we noticed in the project’s test coverage, and what we learned about using AI to help with software development. We also include recommendations for future improvements to the project.
\end{abstract}

\section{Introduction}
For this assignment, we created a custom MCP (Model Context Protocol) server that connects with VS Code Chat and allows us to run tests, commit code, and generate new tests automatically. The main idea behind the project was to explore how automated tools and AI can improve the software development workflow. Since we are still learning about automation and testing, this project helped us understand how tools can speed things up and reduce mistakes we would normally make by hand.

\section{Methodology}
Our approach was pretty straightforward. First, we set up the MCP server and added tools for running Maven tests and Git commands. Then we made a tool that generates JUnit tests based on simple specifications, like checking positive, negative, and zero inputs. After that, we tested everything using the VS Code Chat interface, which let us call the tools by typing messages.

To evaluate how useful the generated tests were, we compared the original test suite with the new tests created by the MCP tool later. We also observed how often the tests caught issues or improved the overall coverage.

\section{Results}
\subsection{Coverage Improvement Patterns}
We noticed some coverage improvements. The original project focused mostly on regular input values, but the automatically generated tests forced the program to handle extreme values like \texttt{Integer.MAX\_VALUE} and \texttt{Integer.MIN\_VALUE}. These edge cases increased branch and statement coverage in parts of the code that were not originally exercised.

We also saw that using AI-generated tests made it easier to fill in missing equivalence classes. For example, the tool consistently created tests for negative, zero, and positive values, which helped reveal patterns in the function’s behavior. This made the overall testing feel more complete, even though the tool was simple.

\subsection{Lessons Learned About AI-Assisted Development}
One of the main things we learned is that AI tools are really good at doing repetitive tasks, like writing basic tests. Instead of us having to write every single test case manually, the MCP tool could generate them instantly. This saved time and also reduced the chances of forgetting edge cases.

We also learned that AI can help students like us understand testing strategies better. For example, by looking at the structure of the tests the tool created, we were able to see how equivalence classes and boundary analysis are supposed to work in practice. Even though the AI is not perfect, it provides a starting point that we can build on.

\subsection{Future Enhancements}
There are several things we would add to improve this project in the future. First, we would expand the test generator tool so that it can analyze the source code and detect functions automatically, instead of supporting only one function name. We would also add more advanced testing strategies, like decision coverage or mutation testing, to improve fault detection. Some issues exist with LLMs where they get stuck in loops or unproductive patterns of thought which cuts into their effectiveness. Implementing some form of monitoring or intervention to detect and correct these patterns would enhance the reliability of the generated tests ideally.

Another improvement would be better integration with GitHub Actions. Right now, the MCP agent can run tests locally, but it would be useful if it could also check CI results or automatically update pull requests.

Finally, adding a reporting feature that shows coverage percentages or failed test patterns would make the tool much more helpful for debugging and learning.

\section{Conclusion}
Overall, this project taught me a lot about automating software development using MCP tools. I was able to see how AI can support testing, improve coverage, and streamline common tasks like running Maven or committing changes. Even as a beginner, I felt like I could build useful automation features with relatively simple code. In the future, expanding the agent with more advanced testing and CI integration would make it even more powerful.

\end{document}